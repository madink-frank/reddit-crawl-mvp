# Design Document

## Overview

Reddit Ghost PublisherëŠ” Reddit ì¸ê¸° ê¸€ì„ ìˆ˜ì§‘í•˜ê³  í•œêµ­ì–´ ìš”ì•½ì„ ìƒì„±í•œ ë’¤ Ghost CMSì— ìë™ ë°œí–‰í•˜ëŠ” MVP ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë¹„ìš©ê³¼ ìš´ì˜ ë³µì¡ë„ë¥¼ ìµœì†Œí™”í•˜ê³  í•˜ë£¨ ë‹¨ìœ„ ë°°ì¹˜ë¡œ ì•ˆì • ë™ì‘ì„ í™•ë³´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. FastAPI + Celery + Redis + PostgreSQLì„ ë‹¨ì¼ ë…¸ë“œ Docker Composeë¡œ ìš´ì˜í•˜ëŠ” ë‹¨ìˆœí•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## Architecture

### High-Level Architecture

```mermaid
graph TB
    subgraph "External Services"
        Reddit[Reddit API]
        OpenAI[OpenAI GPT-4o-mini/GPT-4o]
        Ghost[Ghost Pro]
    end
    
    subgraph "Single Node Docker Compose"
        subgraph "API Layer"
            FastAPI[FastAPI Gateway]
        end
        
        subgraph "Message Queue"
            Redis[(Redis)]
            CeleryBeat[Celery Beat Scheduler]
        end
        
        subgraph "Worker Services"
            Collector[Collector Worker]
            NLPPipe[NLP Pipeline Worker]
            Publisher[Publisher Worker]
        end
        
        subgraph "Data Layer"
            PostgreSQL[(PostgreSQL 15)]
        end
        
        subgraph "Basic Monitoring"
            Metrics[/metrics endpoint]
            Health[/health endpoint]
            Logs[Basic Logging]
        end
    end
    
    subgraph "Notifications"
        Slack[Slack Webhook]
    end
    
    FastAPI --> Redis
    CeleryBeat --> Redis
    Redis --> Collector
    Redis --> NLPPipe
    Redis --> Publisher
    
    Collector --> Reddit
    Collector --> PostgreSQL
    NLPPipe --> OpenAI
    NLPPipe --> PostgreSQL
    Publisher --> Ghost
    Publisher --> PostgreSQL
    
    FastAPI --> Metrics
    FastAPI --> Health
    
    Collector --> Slack
    NLPPipe --> Slack
    Publisher --> Slack
    
    PostgreSQL --> Logs
```

### Simplified Architecture

ì‹œìŠ¤í…œì€ ë‹¨ì¼ ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” 3ê°œì˜ Celery ì›Œì»¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. **Collector Worker**: Reddit API í˜¸ì¶œ ë° ë°ì´í„° ìˆ˜ì§‘
2. **NLP Pipeline Worker**: AI ê¸°ë°˜ ì½˜í…ì¸  ì²˜ë¦¬ ë° ë¶„ì„  
3. **Publisher Worker**: Ghost CMS ë°œí–‰ ë° ì´ë¯¸ì§€ ì²˜ë¦¬

ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” ë™ì¼í•œ Docker Compose í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë©°, ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ë§Œ ì§€ì›í•©ë‹ˆë‹¤.

## Components and Interfaces

### 1. FastAPI Gateway

**ì±…ì„**: API ì—”ë“œí¬ì¸íŠ¸ ì œê³µ, ì¸ì¦, ìš”ì²­ ë¼ìš°íŒ…

**ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸**:
```python
# Health check
GET /health

# Manual trigger endpoints
POST /api/v1/collect/trigger
POST /api/v1/process/trigger  
POST /api/v1/publish/trigger

# Status monitoring
GET /api/v1/status/queues
GET /api/v1/status/workers

# Metrics endpoint
GET /metrics

# Takedown endpoint
POST /api/v1/takedown/{reddit_post_id}
```

**í/ì›Œì»¤ ìƒíƒœ API êµ¬í˜„**:
```python
from celery import current_app

@app.get("/api/v1/status/queues")
async def get_queue_status():
    """Redis ê¸°ë°˜ í ìƒíƒœ ì¡°íšŒ"""
    inspect = current_app.control.inspect()
    
    # í™œì„± ì‘ì—…
    active = inspect.active() or {}
    # ì˜ˆì•½ëœ ì‘ì—…  
    scheduled = inspect.scheduled() or {}
    # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… (Redisì—ì„œ ì§ì ‘ ì¡°íšŒ)
    
    queue_stats = {}
    for queue in ['collect', 'process', 'publish']:
        queue_stats[queue] = {
            'active': sum(len(tasks) for tasks in active.values()),
            'scheduled': sum(len(tasks) for tasks in scheduled.values()),
            'pending': redis_client.llen(queue)  # Redis í ê¸¸ì´
        }
    
    return queue_stats

@app.get("/api/v1/status/workers")
async def get_worker_status():
    """Celery ì›Œì»¤ ìƒíƒœ ì¡°íšŒ"""
    inspect = current_app.control.inspect()
    
    stats = inspect.stats() or {}
    active = inspect.active() or {}
    
    worker_stats = {}
    for worker_name, worker_info in stats.items():
        worker_stats[worker_name] = {
            'status': 'online',
            'active_tasks': len(active.get(worker_name, [])),
            'processed_tasks': worker_info.get('total', {}).get('tasks.collector.collect_reddit_posts', 0),
            'load_avg': worker_info.get('rusage', {}).get('utime', 0)
        }
    
    return worker_stats
```

**ê¸°ìˆ  ìŠ¤íƒ**:
- FastAPI with async/await
- Pydantic models for validation
- í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì¸ì¦
- ê¸°ë³¸ ë©”íŠ¸ë¦­ ë…¸ì¶œ (/metrics)

### 2. Celery Queue System

**í êµ¬ì„±**:
```python
# Redis ë¸Œë¡œì»¤ìš© ë‹¨ìˆœ í ì„¤ì •
CELERY_TASK_ROUTES = {
    'workers.collector.tasks.collect_reddit_posts': {'queue': 'collect'},
    'workers.nlp_pipeline.tasks.process_content_with_ai': {'queue': 'process'},
    'workers.publisher.tasks.publish_to_ghost': {'queue': 'publish'},
}

# ì›Œì»¤ ì‹¤í–‰ ëª…ë ¹
# celery -A app.celery worker -Q collect -c 1
# celery -A app.celery worker -Q process -c 1  
# celery -A app.celery worker -Q publish -c 1
```

**ìŠ¤ì¼€ì¤„ë§**:
- Celery Beatì„ ì‚¬ìš©í•œ ì£¼ê¸°ì  ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- Reddit ìˆ˜ì§‘: ì‹œê°„ë‹¹ 1íšŒ (COLLECT_CRON="0 * * * *")
- ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬: ë§¤ 5ë¶„
- ë°±ì—… ì‘ì—…: ë§¤ì¼ ìƒˆë²½ 4ì‹œ (BACKUP_CRON="0 4 * * *")

### 3. Collector Service

**ì±…ì„**: Reddit API í˜¸ì¶œ, ì½˜í…ì¸  í•„í„°ë§, ì†ë„ ê³„ì‚°

**í•µì‹¬ ê¸°ëŠ¥**:
```python
@celery_app.task(bind=True, max_retries=3)
def collect_reddit_posts(self, subreddits: List[str], sort_type: str = "hot"):
    """
    Redditì—ì„œ ê²Œì‹œê¸€ ìˆ˜ì§‘
    - API ì œí•œ ì¤€ìˆ˜ (100 req/min)
    - NSFW í•„í„°ë§
    - ì†ë„ ê³„ì‚° (score/time)
    - ì¤‘ë³µ ì œê±°
    """
    pass

@celery_app.task(bind=True)
def calculate_velocity(self, post_data: dict):
    """
    ê²Œì‹œê¸€ ì†ë„ ê³„ì‚°
    - ì‹œê°„ë‹¹ ì ìˆ˜ ì¦ê°€ìœ¨
    - ëŒ“ê¸€ ì¦ê°€ìœ¨
    - íŠ¸ë Œë“œ ì ìˆ˜ ì‚°ì¶œ
    """
    pass
```

**Reddit API í†µí•©**:
- PRAW (Python Reddit API Wrapper) ì‚¬ìš©
- OAuth 2.0 ì¸ì¦ (í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ê´€ë¦¬)
- Rate limiting: 60 requests/minute
- Exponential backoff ì¬ì‹œë„ ë¡œì§
- ì¼ì¼ API í˜¸ì¶œ ìƒí•œ ëª¨ë‹ˆí„°ë§

### 4. NLP Pipeline Service

**ì±…ì„**: AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ì„, ìš”ì•½, íƒœê¹…

**í•µì‹¬ ê¸°ëŠ¥**:
```python
@celery_app.task(bind=True, max_retries=3)
def process_content_with_ai(self, post_id: str):
    """
    AI ê¸°ë°˜ ì½˜í…ì¸  ì²˜ë¦¬
    - GPT-4o í•œêµ­ì–´ ìš”ì•½
    - BERTopic íƒœê·¸ ì¶”ì¶œ
    - í˜ì¸ í¬ì¸íŠ¸ ë¶„ì„
    - ì œí’ˆ ì•„ì´ë””ì–´ ì¶”ì¶œ
    """
    pass

@celery_app.task(bind=True)
def extract_topics_llm(self, content: str):
    """
    LLM í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ íƒœê·¸ ì¶”ì¶œ
    - 3-5ê°œ íƒœê·¸ ì¶”ì¶œ
    - ì¼ê´€ëœ í‘œê¸° ê·œì¹™ ì ìš©
    - ê²€ìƒ‰ ìµœì í™”ëœ í‚¤ì›Œë“œ ìƒì„±
    """
    pass
```

**AI í†µí•©**:
- OpenAI GPT-4o-mini (primary) + GPT-4o (fallback)
- ë‚´ë¶€ ì½”ìŠ¤íŠ¸ ë§µì„ í†µí•œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- LLM í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ íƒœê·¸ ì¶”ì¶œ (3-5ê°œ ì œí•œ)
- ì¼ì¼ í† í° ì˜ˆì‚° ëª¨ë‹ˆí„°ë§ ë° ì°¨ë‹¨

### 5. Publisher Service

**ì±…ì„**: Ghost CMS ë°œí–‰, ì´ë¯¸ì§€ ì²˜ë¦¬, í…œí”Œë¦¿ ì ìš©

**í•µì‹¬ ê¸°ëŠ¥**:
```python
@celery_app.task(bind=True, max_retries=5)
def publish_to_ghost(self, processed_content_id: str):
    """
    Ghost CMSì— ì½˜í…ì¸  ë°œí–‰
    - Markdown to HTML ë³€í™˜
    - ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° CDN ì²˜ë¦¬
    - íƒœê·¸ ë§¤í•‘
    - ì €ì‘ê¶Œ í‘œì‹œ ì¶”ê°€
    """
    pass

@celery_app.task(bind=True)
def process_media_content(self, media_urls: List[str]):
    """
    ë¯¸ë””ì–´ ì½˜í…ì¸  ì²˜ë¦¬
    - Reddit ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    - Ghost Images API ì—…ë¡œë“œ
    - CDN URL ìƒì„±
    """
    pass
```

**Ghost CMS í†µí•©**:
- Ghost Admin API v5
- JWT ì¸ì¦ (Admin Keyë¡œ ì„œëª…)
- Article í…œí”Œë¦¿ 1ì¢…ë¥˜ë§Œ ì‚¬ìš© (ê³ ì • ì¶œì²˜ ê³ ì§€ í¬í•¨)

**ê³ ì • ì¶œì²˜ ê³ ì§€ ë¬¸êµ¬**:
```html
<!-- ë³¸ë¬¸ í•˜ë‹¨ì— ìë™ ì‚½ì… -->
<hr>
<p><strong>Source:</strong> <a href="{reddit_url}" target="_blank">Reddit</a></p>
<p><em>Media and usernames belong to their respective owners.</em></p>
<p><em>Takedown requests will be honored.</em></p>
```
- ì´ë¯¸ì§€ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ í›„ Ghost Images API ì—…ë¡œë“œ
- ë¯¸ë””ì–´ ì—†ì„ ì‹œ ê¸°ë³¸ OG ì´ë¯¸ì§€ ì‚¬ìš©

## Data Models

### PostgreSQL Schema

```sql
-- ë©”ì¸ ê²Œì‹œê¸€ í…Œì´ë¸” (MVP ìŠ¤í‚¤ë§ˆ)
CREATE TABLE posts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    reddit_post_id TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    subreddit TEXT NOT NULL,
    score INTEGER DEFAULT 0,
    num_comments INTEGER DEFAULT 0,
    created_ts TIMESTAMPTZ NOT NULL,
    
    -- AI ì²˜ë¦¬ ê²°ê³¼
    summary_ko TEXT,
    tags JSONB, -- 3-5ê°œ íƒœê·¸ ë°°ì—´
    pain_points JSONB, -- JSON ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜
    product_ideas JSONB, -- JSON ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜
    
    -- Ghost ë°œí–‰ ì •ë³´
    ghost_url TEXT,
    
    -- ë©”íƒ€ë°ì´í„°
    content_hash TEXT,
    takedown_status TEXT DEFAULT 'active' CHECK (takedown_status IN ('active', 'takedown_pending', 'removed')),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ë¯¸ë””ì–´ íŒŒì¼ í…Œì´ë¸”
CREATE TABLE media_files (
    id SERIAL PRIMARY KEY,
    post_id UUID REFERENCES posts(id),
    original_url TEXT NOT NULL,
    ghost_url TEXT,
    file_type TEXT,
    file_size INTEGER,
    processed_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì²˜ë¦¬ ë¡œê·¸ í…Œì´ë¸”
CREATE TABLE processing_logs (
    id SERIAL PRIMARY KEY,
    post_id UUID REFERENCES posts(id),
    service_name TEXT NOT NULL,
    status TEXT NOT NULL,
    error_message TEXT,
    processing_time_ms INTEGER,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
CREATE TABLE token_usage (
    id SERIAL PRIMARY KEY,
    post_id UUID REFERENCES posts(id),
    service TEXT NOT NULL, -- 'openai'
    model TEXT NOT NULL, -- 'gpt-4o-mini', 'gpt-4o'
    input_tokens INTEGER DEFAULT 0,
    output_tokens INTEGER DEFAULT 0,
    cost_usd DECIMAL(10,6),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì¸ë±ìŠ¤
CREATE UNIQUE INDEX idx_posts_reddit_post_id ON posts(reddit_post_id);
CREATE INDEX idx_posts_created_ts ON posts(created_ts);
CREATE INDEX idx_posts_subreddit ON posts(subreddit);
CREATE INDEX idx_processing_logs_post_id ON processing_logs(post_id);
CREATE INDEX idx_token_usage_created_at ON token_usage(created_at);

-- updated_at ìë™ ê°±ì‹  íŠ¸ë¦¬ê±°
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_posts_updated_at BEFORE UPDATE ON posts
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

### Redis Data Structures

```python
# í ìƒíƒœ ëª¨ë‹ˆí„°ë§
QUEUE_STATS = {
    'collect:pending': 'list',      # ëŒ€ê¸° ì¤‘ì¸ ìˆ˜ì§‘ ì‘ì—…
    'process:pending': 'list',      # ëŒ€ê¸° ì¤‘ì¸ ì²˜ë¦¬ ì‘ì—…  
    'publish:pending': 'list',      # ëŒ€ê¸° ì¤‘ì¸ ë°œí–‰ ì‘ì—…
}

# API ì œí•œ ì¶”ì  (ë‹¨ìˆœí™”)
RATE_LIMITS = {
    'reddit:daily_calls': 'counter',     # ì¼ì¼ API í˜¸ì¶œ ìˆ˜
    'openai:daily_tokens': 'counter',    # ì¼ì¼ í† í° ì‚¬ìš©ëŸ‰
}

# ê¸°ë³¸ ìºì‹œ
CACHE_KEYS = {
    'subreddit:hot:{name}': 'hash',      # ì„œë¸Œë ˆë”§ í•« ê²Œì‹œê¸€ ìºì‹œ (15ë¶„)
    'config:system': 'hash'              # ì‹œìŠ¤í…œ ì„¤ì • ìºì‹œ
}
```

## Error Handling

### 1. ê³„ì¸µë³„ ì—ëŸ¬ ì²˜ë¦¬

**API Layer (FastAPI)**:
```python
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.utcnow().isoformat(),
            "path": request.url.path
        }
    )

@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    return JSONResponse(
        status_code=422,
        content={
            "error": "Validation failed",
            "details": exc.errors(),
            "timestamp": datetime.utcnow().isoformat()
        }
    )
```

**Worker Layer (Celery)**:
```python
@celery_app.task(bind=True, autoretry_for=(RequestException, OpenAIError), 
                 retry_kwargs={'max_retries': 3, 'countdown': 60})
def resilient_task(self, *args, **kwargs):
    try:
        # ì‘ì—… ì‹¤í–‰
        result = perform_task(*args, **kwargs)
        return result
    except RateLimitError as e:
        # Rate limit ì—ëŸ¬ ì‹œ ë” ê¸´ ëŒ€ê¸°
        raise self.retry(countdown=300, exc=e)
    except CriticalError as e:
        # ì¬ì‹œë„í•˜ì§€ ì•Šì„ ì—ëŸ¬
        logger.error(f"Critical error in task: {e}")
        raise
```

### 2. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—ëŸ¬ ì²˜ë¦¬

**Reddit API**:
- Rate limit: í—¤ë” ê¸°ë°˜ ë™ì  ì œì–´ (ë‚¨ì€ ì¿¼í„° ì¶”ì )
- ì´ˆê³¼ ì˜ˆìƒ ì‹œ: íë¡œ ì§€ì—° ì²˜ë¦¬ ë° ì§€ìˆ˜ ë°±ì˜¤í”„
- ì¸ì¦ ì‹¤íŒ¨: í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ì¬ë¡œë“œ
- ì¼ì¼ ìƒí•œ ë„ë‹¬: ìˆ˜ì§‘ ì¤‘ë‹¨ ë° Slack ì•Œë¦¼

**OpenAI API**:
- GPT-4o-mini ì‹¤íŒ¨: GPT-4oë¡œ í´ë°±
- í† í° ì˜ˆì‚° ì´ˆê³¼: ì¼ì¼ ì˜ˆì‚° ì•Œë¦¼ ë° ì‘ì—… ì°¨ë‹¨
- ëª¨ë¸ ì˜¤ë²„ë¡œë“œ: 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„

**Ghost CMS**:
- ë°œí–‰ ì‹¤íŒ¨: 3íšŒê¹Œì§€ ì¬ì‹œë„ (ì§€ìˆ˜ì  ë°±ì˜¤í”„)
- ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: ê¸°ë³¸ OG ì´ë¯¸ì§€ë¡œ ëŒ€ì²´
- ë¯¸ë””ì–´ ì—†ëŠ” ê²Œì‹œê¸€: ê¸°ë³¸ OG ì´ë¯¸ì§€ ìë™ ì‚½ì…
- ì¤‘ë³µ ë°œí–‰ ë°©ì§€: reddit_post_id ê¸°ë°˜ ì²´í¬ (ë©±ë“±ì„± ë³´ì¥)

### 3. ë°ì´í„° ì¼ê´€ì„± ë³´ì¥

**íŠ¸ëœì­ì…˜ ê´€ë¦¬**:
```python
async def process_post_with_transaction(post_id: str):
    async with database.transaction():
        try:
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            await update_post_status(post_id, 'processing')
            
            # AI ì²˜ë¦¬
            result = await process_with_ai(post_id)
            
            # ê²°ê³¼ ì €ì¥
            await save_processing_result(post_id, result)
            
            # ìƒíƒœ ì™„ë£Œë¡œ ë³€ê²½
            await update_post_status(post_id, 'processed')
            
        except Exception as e:
            # ì‹¤íŒ¨ ìƒíƒœë¡œ ë³€ê²½
            await update_post_status(post_id, 'failed')
            raise
```

## Testing Strategy

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (pytest)

**ì»¤ë²„ë¦¬ì§€ ëª©í‘œ**: 70% ì´ìƒ

```python
# í…ŒìŠ¤íŠ¸ êµ¬ì¡° (ë‹¨ìˆœí™”)
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_collector.py      # Reddit API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_nlp_pipeline.py   # AI ì²˜ë¦¬ ë¡œì§ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_publisher.py      # Ghost ë°œí–‰ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_models.py         # ë°ì´í„° ëª¨ë¸ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_api_endpoints.py  # FastAPI ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_celery_tasks.py   # Celery ì‘ì—… í…ŒìŠ¤íŠ¸
â””â”€â”€ e2e/
    â””â”€â”€ test_smoke_tests.py    # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸
```

**Mock ì „ëµ**:
```python
# ì™¸ë¶€ ì„œë¹„ìŠ¤ Mock
@pytest.fixture
def mock_reddit_api():
    with patch('praw.Reddit') as mock:
        mock.return_value.subreddit.return_value.hot.return_value = [
            MockSubmission(id='test1', title='Test Post', score=100)
        ]
        yield mock

@pytest.fixture  
def mock_openai_api():
    with patch('openai.ChatCompletion.create') as mock:
        mock.return_value = {
            'choices': [{'message': {'content': 'Test summary'}}],
            'usage': {'prompt_tokens': 100, 'completion_tokens': 50}
        }
        yield mock
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸ (Postman/Newman)

**API í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:
- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
- ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
- ìˆ˜ë™ íŠ¸ë¦¬ê±° ì—”ë“œí¬ì¸íŠ¸ (ìˆ˜ì§‘â†’ìš”ì•½â†’ë°œí–‰)
- ì¤‘ë³µ ë°œí–‰ ë°©ì§€ í…ŒìŠ¤íŠ¸ (ë™ì¼ reddit_post_id ì¬ì‹œë„ ì‹œ 409/ë©±ë“± ì²˜ë¦¬)

**ëª©í‘œ**: 100% ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ í†µê³¼

### 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (k6)

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:
```javascript
import http from 'k6/http';
import { check } from 'k6';

export const options = {
  vus: 50,
  duration: '5m',
  thresholds: {
    http_req_duration: ['p(95)<300'],
    http_req_failed: ['rate<0.05'],
  },
};

export default function () {
  const res = http.get(__ENV.API_BASE + '/health');
  check(res, { 'status 200': (r) => r.status === 200 });
}
```

**ì„±ëŠ¥ ëª©í‘œ**:
- p95 ì‘ë‹µì‹œê°„ < 300ms
- E2E ì²˜ë¦¬ ì‹œê°„ < 5ë¶„
- ì‹œê°„ë‹¹ 100ê°œ ê¸€ ì²˜ë¦¬

## Monitoring and Observability

### 1. Basic Metrics

**ê¸°ë³¸ ë©”íŠ¸ë¦­ (DB ê¸°ë°˜ ì§‘ê³„)**:
```python
# ì›Œì»¤ëŠ” DBì— ì¹´ìš´í„° ì €ì¥, FastAPIê°€ /metricsì—ì„œ ì§‘ê³„
def get_metrics_from_db():
    """DBì—ì„œ ë©”íŠ¸ë¦­ ì§‘ê³„í•˜ì—¬ Prometheus í¬ë§·ìœ¼ë¡œ ë°˜í™˜"""
    with get_db_session() as session:
        # ì¼ì¼ ìˆ˜ì§‘/ì²˜ë¦¬/ë°œí–‰ ê±´ìˆ˜
        today = datetime.utcnow().date()
        
        collected = session.query(ProcessingLog).filter(
            ProcessingLog.service_name == 'collector',
            ProcessingLog.status == 'success',
            func.date(ProcessingLog.created_at) == today
        ).count()
        
        processed = session.query(ProcessingLog).filter(
            ProcessingLog.service_name == 'nlp_pipeline',
            ProcessingLog.status == 'success',
            func.date(ProcessingLog.created_at) == today
        ).count()
        
        published = session.query(ProcessingLog).filter(
            ProcessingLog.service_name == 'publisher',
            ProcessingLog.status == 'success',
            func.date(ProcessingLog.created_at) == today
        ).count()
        
        failures = session.query(ProcessingLog).filter(
            ProcessingLog.status == 'failed',
            func.date(ProcessingLog.created_at) == today
        ).count()
        
        return f"""# HELP reddit_posts_collected_total Total Reddit posts collected
# TYPE reddit_posts_collected_total counter
reddit_posts_collected_total {collected}

# HELP posts_processed_total Total posts processed
# TYPE posts_processed_total counter
posts_processed_total {processed}

# HELP posts_published_total Total posts published
# TYPE posts_published_total counter
posts_published_total {published}

# HELP processing_failures_total Total processing failures
# TYPE processing_failures_total counter
processing_failures_total {failures}
"""
```

### 2. Health Check

**í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸**:
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "services": {
            "database": await check_database(),
            "redis": await check_redis(),
            "external_apis": await check_external_apis()
        }
    }
```

### 3. Slack Notifications

**ì•Œë¦¼ ì¡°ê±´**:
- ì‹¤íŒ¨ìœ¨ > 5%
- í ëŒ€ê¸° ìˆ˜ > 500
- ì¼ì¼ API í˜¸ì¶œ 80% ë„ë‹¬
- ì¼ì¼ í† í° ì˜ˆì‚° 80% ë„ë‹¬

**Slack ì•Œë¦¼ í…œí”Œë¦¿ í†µì¼**:
```python
def send_slack_alert(severity: str, service: str, message: str, metrics: dict = None):
    """í†µì¼ëœ Slack ì•Œë¦¼ í…œí”Œë¦¿"""
    payload = {
        "text": f"ğŸš¨ [{severity}] {service} Alert",
        "attachments": [
            {
                "color": "danger" if severity == "HIGH" else "warning",
                "fields": [
                    {"title": "Service", "value": service, "short": True},
                    {"title": "Message", "value": message, "short": False}
                ]
            }
        ]
    }
    
    if metrics:
        metric_fields = [
            {"title": k, "value": str(v), "short": True} 
            for k, v in metrics.items()
        ]
        payload["attachments"][0]["fields"].extend(metric_fields)
    
    requests.post(SLACK_WEBHOOK_URL, json=payload)

def send_daily_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸"""
    report = {
        "collected_posts": get_daily_collected_count(),
        "published_posts": get_daily_published_count(), 
        "token_usage": get_daily_token_usage(),
        "cost_estimate": calculate_daily_cost()
    }
    
    payload = {
        "text": "ğŸ“Š Daily Reddit Publisher Report",
        "attachments": [
            {
                "color": "good",
                "fields": [
                    {"title": "Posts Collected", "value": str(report["collected_posts"]), "short": True},
                    {"title": "Posts Published", "value": str(report["published_posts"]), "short": True},
                    {"title": "Token Usage", "value": f"{report['token_usage']:,}", "short": True},
                    {"title": "Est. Cost", "value": f"${report['cost_estimate']:.2f}", "short": True}
                ]
            }
        ]
    }
    
    requests.post(SLACK_WEBHOOK_URL, json=payload)
```

### 4. Basic Logging

**ë¡œê·¸ êµ¬ì¡°í™”**:
```python
import logging
import json

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ë¡œê¹… (PII ë§ˆìŠ¤í‚¹)
def log_reddit_collection(post_id, subreddit, score):
    logger.info(json.dumps({
        "event": "reddit_post_collected",
        "post_id": post_id,
        "subreddit": subreddit,
        "score": score,
        "timestamp": datetime.utcnow().isoformat()
    }))

def log_api_error(service, error_message, retry_count):
    logger.error(json.dumps({
        "event": "api_error",
        "service": service,
        "error": mask_sensitive_data(error_message),
        "retry_count": retry_count,
        "timestamp": datetime.utcnow().isoformat()
    }))
```

## Security Architecture

### 1. í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë¹„ë°€ ê´€ë¦¬

**í™˜ê²½ë³€ìˆ˜ êµ¬ì¡°**:
```bash
# Reddit API
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=your_user_agent
REDDIT_DAILY_CALLS_LIMIT=5000

# OpenAI API
OPENAI_API_KEY=your_openai_key
OPENAI_DAILY_TOKENS_LIMIT=100000

# Ghost CMS
GHOST_ADMIN_KEY=your_ghost_admin_key
GHOST_API_URL=https://your-blog.ghost.io

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/reddit_publisher
REDIS_URL=redis://localhost:6379

# Slack (í†µì¼ëœ ì•Œë¦¼ í…œí”Œë¦¿)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# ìŠ¤ì¼€ì¤„ë§
COLLECT_CRON=0 * * * *  # ì‹œê°„ë‹¹ 1íšŒ (ìš´ì˜ ê¸°ë³¸)
BACKUP_CRON=0 4 * * *   # ë§¤ì¼ ìƒˆë²½ 4ì‹œ

# ì•Œë¦¼ ì„ê³„ì¹˜
QUEUE_ALERT_THRESHOLD=500
FAILURE_RATE_THRESHOLD=0.05
```

**ë¹„ë°€ ì •ë³´ ë¡œë”©**:
```python
import os
from functools import lru_cache

@lru_cache()
def get_settings():
    return {
        'reddit_client_id': os.getenv('REDDIT_CLIENT_ID'),
        'reddit_client_secret': os.getenv('REDDIT_CLIENT_SECRET'),
        'openai_api_key': os.getenv('OPENAI_API_KEY'),
        'ghost_admin_key': os.getenv('GHOST_ADMIN_KEY'),
        'slack_webhook_url': os.getenv('SLACK_WEBHOOK_URL')
    }
```

### 2. ë¡œê·¸ ë§ˆìŠ¤í‚¹ ë° PII ë³´í˜¸

**ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹**:
```python
import re

def mask_sensitive_data(text: str) -> str:
    """ë¡œê·¸ì—ì„œ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
    # API í‚¤ ë§ˆìŠ¤í‚¹
    text = re.sub(r'(api[_-]?key["\s]*[:=]["\s]*)([a-zA-Z0-9-_]{10,})', r'\1****', text, flags=re.IGNORECASE)
    # í† í° ë§ˆìŠ¤í‚¹
    text = re.sub(r'(token["\s]*[:=]["\s]*)([a-zA-Z0-9-_]{10,})', r'\1****', text, flags=re.IGNORECASE)
    # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
    text = re.sub(r'([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', r'****@\2', text)
    return text

def log_with_masking(message: str, **kwargs):
    """ë§ˆìŠ¤í‚¹ëœ ë¡œê·¸ ì¶œë ¥"""
    masked_message = mask_sensitive_data(message)
    masked_kwargs = {k: mask_sensitive_data(str(v)) if isinstance(v, str) else v 
                    for k, v in kwargs.items()}
    logger.info(masked_message, **masked_kwargs)
```

**Ghost Admin API JWT ì¸ì¦**:
```python
import jwt
import time

def generate_ghost_jwt(admin_key: str) -> str:
    """Ghost Admin Keyë¡œ JWT ìƒì„±"""
    # Admin Key í˜•ì‹: key_id:secret
    key_id, secret = admin_key.split(':')
    
    iat = int(time.time())
    exp = iat + 300  # 5ë¶„ ë§Œë£Œ
    
    payload = {
        'iat': iat,
        'exp': exp,
        'aud': '/admin/'
    }
    
    token = jwt.encode(payload, bytes.fromhex(secret), algorithm='HS256', headers={'kid': key_id})
    return token

def get_ghost_headers(admin_key: str) -> dict:
    """Ghost API ìš”ì²­ í—¤ë” ìƒì„±"""
    jwt_token = generate_ghost_jwt(admin_key)
    return {
        'Authorization': f'Ghost {jwt_token}',
        'Content-Type': 'application/json'
    }
```

### 3. Takedown ì›Œí¬í”Œë¡œ (2ë‹¨ê³„)

**ê¶Œë¦¬ì ì‚­ì œ ìš”ì²­ ì²˜ë¦¬**:
```python
def handle_takedown_request(reddit_post_id: str, reason: str):
    """ì¦‰ì‹œ ë¹„ê³µê°œ â†’ 72ì‹œê°„ ë‚´ ì‚­ì œ (2ë‹¨ê³„)"""
    try:
        # 1ë‹¨ê³„: ì¦‰ì‹œ ë¹„ê³µê°œ ì²˜ë¦¬
        post = get_post_by_reddit_id(reddit_post_id)
        if post and post.ghost_url:
            ghost_client.unpublish_post(post.ghost_id)
        
        # ìƒíƒœë¥¼ takedown_pendingìœ¼ë¡œ ë³€ê²½
        update_post_status(reddit_post_id, 'takedown_pending')
        
        # ê°ì‚¬ ë¡œê·¸ ê¸°ë¡ (1ë‹¨ê³„)
        audit_log = {
            "event": "takedown_request_received",
            "reddit_post_id": reddit_post_id,
            "reason": reason,
            "action": "unpublished_immediately",
            "processed_at": datetime.utcnow().isoformat(),
            "deletion_scheduled": (datetime.utcnow() + timedelta(hours=72)).isoformat()
        }
        logger.info(json.dumps(audit_log))
        
        # 2ë‹¨ê³„: 72ì‹œê°„ í›„ ì‚­ì œ ìŠ¤ì¼€ì¤„ë§
        schedule_deletion.apply_async(
            args=[reddit_post_id, reason],
            countdown=72 * 3600  # 72ì‹œê°„ í›„
        )
        
        return {"status": "unpublished", "message": "Content unpublished, deletion scheduled in 72h"}
    
    except Exception as e:
        logger.error(f"Takedown request failed: {mask_sensitive_data(str(e))}")
        raise

@celery_app.task
def schedule_deletion(reddit_post_id: str, reason: str):
    """72ì‹œê°„ í›„ ì‹¤ì œ ì‚­ì œ"""
    try:
        delete_post(reddit_post_id)
        
        # ê°ì‚¬ ë¡œê·¸ ê¸°ë¡ (2ë‹¨ê³„)
        audit_log = {
            "event": "takedown_deletion_completed",
            "reddit_post_id": reddit_post_id,
            "reason": reason,
            "deleted_at": datetime.utcnow().isoformat(),
            "sla_met": True
        }
        logger.info(json.dumps(audit_log))
        
    except Exception as e:
        logger.error(f"Scheduled deletion failed: {mask_sensitive_data(str(e))}")
```

## Deployment Architecture

### 1. ë‹¨ì¼ ë…¸ë“œ Docker Compose êµ¬ì„±

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - REDDIT_CLIENT_ID=${REDDIT_CLIENT_ID}
      - REDDIT_CLIENT_SECRET=${REDDIT_CLIENT_SECRET}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GHOST_ADMIN_KEY=${GHOST_ADMIN_KEY}
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
  
  worker-collector:
    build: .
    command: celery -A app.celery worker -Q collect -c 1
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - REDDIT_CLIENT_ID=${REDDIT_CLIENT_ID}
      - REDDIT_CLIENT_SECRET=${REDDIT_CLIENT_SECRET}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
  
  worker-nlp:
    build: .
    command: celery -A app.celery worker -Q process -c 1
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
  
  worker-publisher:
    build: .
    command: celery -A app.celery worker -Q publish -c 1
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - GHOST_ADMIN_KEY=${GHOST_ADMIN_KEY}
      - GHOST_API_URL=${GHOST_API_URL}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
  
  scheduler:
    build: .
    command: celery -A app.celery beat
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - COLLECT_CRON=${COLLECT_CRON:-0 * * * *}
      - BACKUP_CRON=${BACKUP_CRON:-0 4 * * *}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
  
  backup:
    image: postgres:15-alpine
    command: >
      sh -c "
        echo '${BACKUP_CRON:-0 4 * * *} /usr/local/bin/backup-database.sh' | crontab - &&
        crond -f
      "
    environment:
      - PGHOST=postgres
      - PGUSER=${DB_USER}
      - PGPASSWORD=${DB_PASSWORD}
      - PGDATABASE=reddit_publisher
    volumes:
      - ./scripts/backup-database.sh:/usr/local/bin/backup-database.sh:ro
      - postgres_backups:/backups
    depends_on:
      - postgres
    restart: unless-stopped
  
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=reddit_publisher
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    ports:
      - "5432:5432"
    restart: unless-stopped
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  postgres_backups:
```

### 2. ë°±ì—… ë° ë³µêµ¬

**PostgreSQL ë°±ì—… ìŠ¤í¬ë¦½íŠ¸**:
```bash
#!/bin/bash
# backup-database.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups"
DB_NAME="reddit_publisher"

# pg_dump ë°±ì—… ìƒì„±
pg_dump -h postgres -U ${DB_USER} -d ${DB_NAME} > ${BACKUP_DIR}/backup_${DATE}.sql

# 7ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
find ${BACKUP_DIR} -name "backup_*.sql" -mtime +7 -delete

echo "Backup completed: backup_${DATE}.sql"
```

**ë³µêµ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**:
```bash
#!/bin/bash
# restore-test.sh

BACKUP_FILE=$1
TEST_DB="reddit_publisher_test"

# í…ŒìŠ¤íŠ¸ DB ìƒì„±
createdb -h postgres -U ${DB_USER} ${TEST_DB}

# ë°±ì—… ë³µêµ¬
psql -h postgres -U ${DB_USER} -d ${TEST_DB} < ${BACKUP_FILE}

# ë°ì´í„° ê²€ì¦
psql -h postgres -U ${DB_USER} -d ${TEST_DB} -c "SELECT COUNT(*) FROM posts;"

# í…ŒìŠ¤íŠ¸ DB ì‚­ì œ
dropdb -h postgres -U ${DB_USER} ${TEST_DB}

echo "Restore test completed successfully"
```

### 3. CI/CD íŒŒì´í”„ë¼ì¸ (GitHub Actions)

```yaml
name: Reddit Publisher CI/CD

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run tests
      run: |
        pytest --cov=app --cov-report=xml --cov-fail-under=70
    
    - name: Weekly backup restore test
      if: github.event.schedule == '0 2 * * 1'  # ë§¤ì£¼ ì›”ìš”ì¼ 02:00
      run: |
        # ìµœì‹  ë°±ì—…ìœ¼ë¡œ ë³µêµ¬ í…ŒìŠ¤íŠ¸
        ./scripts/restore-test.sh /backups/$(ls -t /backups/backup_*.sql | head -1)
    
    - name: Build Docker image
      run: |
        docker build -t reddit-publisher:${{ github.sha }} .
    
    - name: Run Postman smoke tests
      run: |
        docker-compose -f docker-compose.test.yml up -d
        sleep 30
        newman run tests/postman/smoke-tests.json --environment tests/postman/test-env.json
        docker-compose -f docker-compose.test.yml down
  
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Manual approval required
      uses: trstringer/manual-approval@v1
      with:
        secret: ${{ github.TOKEN }}
        approvers: maintainer1,maintainer2
        minimum-approvals: 1
        issue-title: "Deploy Reddit Publisher to Production"
    
    - name: Deploy to production
      run: |
        # ìˆ˜ë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        ./scripts/deploy.sh ${{ github.sha }}
    
    - name: Health check
      run: |
        sleep 30
        curl -f ${{ secrets.API_BASE_URL }}/health || exit 1
```

ì´ ì„¤ê³„ëŠ” MVP ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ë‹¨ìˆœí™”ë˜ì—ˆìœ¼ë©°, ë¹„ìš©ê³¼ ìš´ì˜ ë³µì¡ë„ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„ ì•ˆì •ì ì¸ ìš´ì˜ì´ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.