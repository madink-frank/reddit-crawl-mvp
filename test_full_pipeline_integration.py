#!/usr/bin/env python3
"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
Reddit ìˆ˜ì§‘ â†’ AI ì²˜ë¦¬ â†’ Ghost ë°œí–‰ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
"""

import os
import sys
import time
import json
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# API ì„¤ì •
API_BASE_URL = "http://localhost:8000"
API_KEY = "reddit-publisher-api-key-2024"
HEADERS = {
    "X-API-Key": API_KEY,
    "Content-Type": "application/json"
}

def print_header(title: str):
    """í…ŒìŠ¤íŠ¸ ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ {title}")
    print(f"{'='*70}")

def print_step(step: str):
    """í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ì¶œë ¥"""
    print(f"\nğŸ” {step}")
    print("-" * 50)

def print_substep(substep: str):
    """í…ŒìŠ¤íŠ¸ í•˜ìœ„ ë‹¨ê³„ ì¶œë ¥"""
    print(f"   ğŸ“‹ {substep}")

def check_system_health() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì „ì²´ í—¬ìŠ¤ì²´í¬"""
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=10)
        if response.status_code == 200:
            health_data = response.json()
            print(f"âœ… System Status: {health_data.get('status', 'unknown')}")
            
            services = health_data.get('services', {})
            all_healthy = True
            
            for service_name, service_info in services.items():
                status = service_info.get('status', 'unknown')
                response_time = service_info.get('response_time_ms', 0)
                
                if status == "healthy":
                    emoji = "âœ…"
                elif status == "degraded":
                    emoji = "âš ï¸"
                    # Ghost APIê°€ rate limitedì—¬ë„ í…ŒìŠ¤íŠ¸ ì§„í–‰ ê°€ëŠ¥
                    if service_name != "ghost_api":
                        all_healthy = False
                else:
                    emoji = "âŒ"
                    all_healthy = False
                
                print(f"   {emoji} {service_name}: {status} ({response_time:.1f}ms)")
            
            return {
                "healthy": all_healthy,
                "services": services,
                "response": health_data
            }
        else:
            print(f"âŒ Health Check Failed: {response.status_code}")
            return {"healthy": False, "error": f"HTTP {response.status_code}"}
    except Exception as e:
        print(f"âŒ Health Check Error: {e}")
        return {"healthy": False, "error": str(e)}

def get_initial_metrics() -> Dict[str, int]:
    """ì´ˆê¸° ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    try:
        response = requests.get(f"{API_BASE_URL}/metrics", headers=HEADERS, timeout=10)
        if response.status_code == 200:
            metrics_text = response.text
            
            # ë©”íŠ¸ë¦­ íŒŒì‹±
            metrics = {
                "posts_collected": 0,
                "posts_processed": 0,
                "posts_published": 0,
                "collection_failures": 0,
                "processing_failures": 0,
                "publishing_failures": 0
            }
            
            for line in metrics_text.split('\n'):
                if line.startswith('#') or not line.strip():
                    continue
                
                for metric_name in metrics.keys():
                    if metric_name in line:
                        try:
                            value = int(line.split()[-1])
                            metrics[metric_name] = value
                            break
                        except (ValueError, IndexError):
                            pass
            
            print(f"ğŸ“Š Initial Metrics:")
            for metric, value in metrics.items():
                print(f"   {metric}: {value}")
            
            return metrics
        else:
            print(f"âš ï¸ Failed to get initial metrics: {response.status_code}")
            return {}
    except Exception as e:
        print(f"âš ï¸ Initial metrics error: {e}")
        return {}

def trigger_reddit_collection() -> Dict[str, Any]:
    """Reddit ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
    try:
        print_substep("Triggering Reddit collection...")
        
        response = requests.post(
            f"{API_BASE_URL}/api/v1/collect/trigger",
            headers=HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Collection triggered successfully")
            print(f"      Task ID: {result.get('task_id')}")
            print(f"      Message: {result.get('message')}")
            return result
        else:
            print(f"   âŒ Collection trigger failed: {response.status_code}")
            print(f"      Response: {response.text}")
            return {}
    except Exception as e:
        print(f"   âŒ Collection trigger error: {e}")
        return {}

def wait_for_collection_completion(max_wait_seconds: int = 300) -> bool:
    """Reddit ìˆ˜ì§‘ ì™„ë£Œ ëŒ€ê¸°"""
    print_substep(f"Waiting for collection completion (max {max_wait_seconds}s)...")
    
    start_time = time.time()
    last_collected_count = 0
    stable_count = 0
    
    while time.time() - start_time < max_wait_seconds:
        try:
            # í ìƒíƒœ í™•ì¸
            response = requests.get(f"{API_BASE_URL}/api/v1/status/queues", headers=HEADERS, timeout=10)
            if response.status_code == 200:
                queue_status = response.json()
                collect_queue = queue_status.get('collect', {})
                
                active = collect_queue.get('active', 0)
                pending = collect_queue.get('pending', 0)
                
                # ë©”íŠ¸ë¦­ì—ì„œ ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ ìˆ˜ í™•ì¸
                metrics_response = requests.get(f"{API_BASE_URL}/metrics", headers=HEADERS, timeout=10)
                collected_count = 0
                if metrics_response.status_code == 200:
                    for line in metrics_response.text.split('\n'):
                        if 'posts_collected_total' in line and not line.startswith('#'):
                            try:
                                collected_count = int(line.split()[-1])
                                break
                            except (ValueError, IndexError):
                                pass
                
                print(f"      â³ Collection status - Active: {active}, Pending: {pending}, Collected: {collected_count}")
                
                # ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸ (íê°€ ë¹„ì–´ìˆê³  ìˆ˜ì§‘ ìˆ˜ê°€ ì•ˆì •ì )
                if active == 0 and pending == 0:
                    if collected_count == last_collected_count:
                        stable_count += 1
                        if stable_count >= 3:  # 3ë²ˆ ì—°ì† ì•ˆì •ì ì´ë©´ ì™„ë£Œë¡œ ê°„ì£¼
                            print(f"   âœ… Collection completed - {collected_count} posts collected")
                            return True
                    else:
                        stable_count = 0
                        last_collected_count = collected_count
                else:
                    stable_count = 0
            else:
                print(f"      âš ï¸ Queue status check failed: {response.status_code}")
        except Exception as e:
            print(f"      âš ï¸ Collection status error: {e}")
        
        time.sleep(10)
    
    print(f"   âš ï¸ Collection did not complete within {max_wait_seconds} seconds")
    return False

def trigger_ai_processing() -> Dict[str, Any]:
    """AI ì²˜ë¦¬ íŠ¸ë¦¬ê±°"""
    try:
        print_substep("Triggering AI processing...")
        
        response = requests.post(
            f"{API_BASE_URL}/api/v1/process/trigger",
            headers=HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… AI processing triggered successfully")
            print(f"      Task ID: {result.get('task_id')}")
            print(f"      Message: {result.get('message')}")
            return result
        else:
            print(f"   âŒ AI processing trigger failed: {response.status_code}")
            print(f"      Response: {response.text}")
            return {}
    except Exception as e:
        print(f"   âŒ AI processing trigger error: {e}")
        return {}

def wait_for_processing_completion(max_wait_seconds: int = 600) -> bool:
    """AI ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°"""
    print_substep(f"Waiting for AI processing completion (max {max_wait_seconds}s)...")
    
    start_time = time.time()
    last_processed_count = 0
    stable_count = 0
    
    while time.time() - start_time < max_wait_seconds:
        try:
            # í ìƒíƒœ í™•ì¸
            response = requests.get(f"{API_BASE_URL}/api/v1/status/queues", headers=HEADERS, timeout=10)
            if response.status_code == 200:
                queue_status = response.json()
                process_queue = queue_status.get('process', {})
                
                active = process_queue.get('active', 0)
                pending = process_queue.get('pending', 0)
                
                # ë©”íŠ¸ë¦­ì—ì„œ ì²˜ë¦¬ëœ í¬ìŠ¤íŠ¸ ìˆ˜ í™•ì¸
                metrics_response = requests.get(f"{API_BASE_URL}/metrics", headers=HEADERS, timeout=10)
                processed_count = 0
                if metrics_response.status_code == 200:
                    for line in metrics_response.text.split('\n'):
                        if 'posts_processed_total' in line and not line.startswith('#'):
                            try:
                                processed_count = int(line.split()[-1])
                                break
                            except (ValueError, IndexError):
                                pass
                
                print(f"      â³ Processing status - Active: {active}, Pending: {pending}, Processed: {processed_count}")
                
                # ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                if active == 0 and pending == 0:
                    if processed_count == last_processed_count:
                        stable_count += 1
                        if stable_count >= 3:
                            print(f"   âœ… AI processing completed - {processed_count} posts processed")
                            return True
                    else:
                        stable_count = 0
                        last_processed_count = processed_count
                else:
                    stable_count = 0
            else:
                print(f"      âš ï¸ Queue status check failed: {response.status_code}")
        except Exception as e:
            print(f"      âš ï¸ Processing status error: {e}")
        
        time.sleep(15)  # AI ì²˜ë¦¬ëŠ” ë” ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ 15ì´ˆ ê°„ê²©
    
    print(f"   âš ï¸ AI processing did not complete within {max_wait_seconds} seconds")
    return False

def trigger_ghost_publishing() -> Dict[str, Any]:
    """Ghost ë°œí–‰ íŠ¸ë¦¬ê±°"""
    try:
        print_substep("Triggering Ghost publishing...")
        
        response = requests.post(
            f"{API_BASE_URL}/api/v1/publish/trigger",
            headers=HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Ghost publishing triggered successfully")
            print(f"      Task ID: {result.get('task_id')}")
            print(f"      Message: {result.get('message')}")
            return result
        else:
            print(f"   âŒ Ghost publishing trigger failed: {response.status_code}")
            print(f"      Response: {response.text}")
            return {}
    except Exception as e:
        print(f"   âŒ Ghost publishing trigger error: {e}")
        return {}

def wait_for_publishing_completion(max_wait_seconds: int = 300) -> bool:
    """Ghost ë°œí–‰ ì™„ë£Œ ëŒ€ê¸°"""
    print_substep(f"Waiting for Ghost publishing completion (max {max_wait_seconds}s)...")
    
    start_time = time.time()
    last_published_count = 0
    stable_count = 0
    
    while time.time() - start_time < max_wait_seconds:
        try:
            # í ìƒíƒœ í™•ì¸
            response = requests.get(f"{API_BASE_URL}/api/v1/status/queues", headers=HEADERS, timeout=10)
            if response.status_code == 200:
                queue_status = response.json()
                publish_queue = queue_status.get('publish', {})
                
                active = publish_queue.get('active', 0)
                pending = publish_queue.get('pending', 0)
                
                # ë©”íŠ¸ë¦­ì—ì„œ ë°œí–‰ëœ í¬ìŠ¤íŠ¸ ìˆ˜ í™•ì¸
                metrics_response = requests.get(f"{API_BASE_URL}/metrics", headers=HEADERS, timeout=10)
                published_count = 0
                if metrics_response.status_code == 200:
                    for line in metrics_response.text.split('\n'):
                        if 'posts_published_total' in line and not line.startswith('#'):
                            try:
                                published_count = int(line.split()[-1])
                                break
                            except (ValueError, IndexError):
                                pass
                
                print(f"      â³ Publishing status - Active: {active}, Pending: {pending}, Published: {published_count}")
                
                # ë°œí–‰ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                if active == 0 and pending == 0:
                    if published_count == last_published_count:
                        stable_count += 1
                        if stable_count >= 3:
                            print(f"   âœ… Ghost publishing completed - {published_count} posts published")
                            return True
                    else:
                        stable_count = 0
                        last_published_count = published_count
                else:
                    stable_count = 0
            else:
                print(f"      âš ï¸ Queue status check failed: {response.status_code}")
        except Exception as e:
            print(f"      âš ï¸ Publishing status error: {e}")
        
        time.sleep(10)
    
    print(f"   âš ï¸ Ghost publishing did not complete within {max_wait_seconds} seconds")
    return False

def get_final_metrics() -> Dict[str, int]:
    """ìµœì¢… ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    try:
        response = requests.get(f"{API_BASE_URL}/metrics", headers=HEADERS, timeout=10)
        if response.status_code == 200:
            metrics_text = response.text
            
            metrics = {
                "posts_collected": 0,
                "posts_processed": 0,
                "posts_published": 0,
                "collection_failures": 0,
                "processing_failures": 0,
                "publishing_failures": 0
            }
            
            for line in metrics_text.split('\n'):
                if line.startswith('#') or not line.strip():
                    continue
                
                for metric_name in metrics.keys():
                    if metric_name in line:
                        try:
                            value = int(line.split()[-1])
                            metrics[metric_name] = value
                            break
                        except (ValueError, IndexError):
                            pass
            
            print(f"ğŸ“Š Final Metrics:")
            for metric, value in metrics.items():
                print(f"   {metric}: {value}")
            
            return metrics
        else:
            print(f"âš ï¸ Failed to get final metrics: {response.status_code}")
            return {}
    except Exception as e:
        print(f"âš ï¸ Final metrics error: {e}")
        return {}

def verify_pipeline_results(initial_metrics: Dict[str, int], final_metrics: Dict[str, int]) -> Dict[str, Any]:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê²€ì¦"""
    print_substep("Verifying pipeline results...")
    
    results = {
        "collection_success": False,
        "processing_success": False,
        "publishing_success": False,
        "overall_success": False,
        "stats": {}
    }
    
    # ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€
    collected_delta = final_metrics.get('posts_collected', 0) - initial_metrics.get('posts_collected', 0)
    collection_failures = final_metrics.get('collection_failures', 0) - initial_metrics.get('collection_failures', 0)
    
    if collected_delta > 0:
        results["collection_success"] = True
        print(f"   âœ… Collection: {collected_delta} new posts collected")
    else:
        print(f"   âŒ Collection: No new posts collected")
    
    if collection_failures > 0:
        print(f"   âš ï¸ Collection failures: {collection_failures}")
    
    # ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
    processed_delta = final_metrics.get('posts_processed', 0) - initial_metrics.get('posts_processed', 0)
    processing_failures = final_metrics.get('processing_failures', 0) - initial_metrics.get('processing_failures', 0)
    
    if processed_delta > 0:
        results["processing_success"] = True
        print(f"   âœ… Processing: {processed_delta} posts processed")
    else:
        print(f"   âŒ Processing: No posts processed")
    
    if processing_failures > 0:
        print(f"   âš ï¸ Processing failures: {processing_failures}")
    
    # ë°œí–‰ ì„±ê³µ ì—¬ë¶€
    published_delta = final_metrics.get('posts_published', 0) - initial_metrics.get('posts_published', 0)
    publishing_failures = final_metrics.get('publishing_failures', 0) - initial_metrics.get('publishing_failures', 0)
    
    if published_delta > 0:
        results["publishing_success"] = True
        print(f"   âœ… Publishing: {published_delta} posts published")
    else:
        print(f"   âŒ Publishing: No posts published")
    
    if publishing_failures > 0:
        print(f"   âš ï¸ Publishing failures: {publishing_failures}")
    
    # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
    success_rate = 0
    if collected_delta > 0:
        success_rate = (published_delta / collected_delta) * 100
    
    results["stats"] = {
        "collected": collected_delta,
        "processed": processed_delta,
        "published": published_delta,
        "success_rate": success_rate,
        "collection_failures": collection_failures,
        "processing_failures": processing_failures,
        "publishing_failures": publishing_failures
    }
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€ (ìµœì†Œ 1ê°œ í¬ìŠ¤íŠ¸ê°€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í†µê³¼í–ˆëŠ”ì§€)
    results["overall_success"] = published_delta > 0
    
    print(f"   ğŸ“ˆ Pipeline Success Rate: {success_rate:.1f}%")
    
    return results

def check_data_quality() -> Dict[str, Any]:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    print_substep("Checking data quality...")
    
    # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ë¡œ í’ˆì§ˆ í™•ì¸
    try:
        import subprocess
        
        # ìµœê·¼ ë°œí–‰ëœ í¬ìŠ¤íŠ¸ í™•ì¸
        cmd = [
            "docker", "exec", "reddit-publisher-postgres",
            "psql", "-U", "reddit_publisher", "-d", "reddit_publisher", "-t", "-c",
            """
            SELECT 
                COUNT(*) as total_posts,
                COUNT(CASE WHEN summary_ko IS NOT NULL AND summary_ko != '' THEN 1 END) as posts_with_summary,
                COUNT(CASE WHEN tags IS NOT NULL AND tags != '[]' THEN 1 END) as posts_with_tags,
                COUNT(CASE WHEN ghost_url IS NOT NULL THEN 1 END) as posts_with_ghost_url,
                COUNT(CASE WHEN status = 'published' THEN 1 END) as published_posts
            FROM posts 
            WHERE created_at >= NOW() - INTERVAL '1 hour';
            """
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            output = result.stdout.strip()
            if output:
                parts = output.split('|')
                if len(parts) >= 5:
                    total = int(parts[0].strip())
                    with_summary = int(parts[1].strip())
                    with_tags = int(parts[2].strip())
                    with_ghost_url = int(parts[3].strip())
                    published = int(parts[4].strip())
                    
                    print(f"   ğŸ“Š Data Quality (last hour):")
                    print(f"      Total posts: {total}")
                    print(f"      Posts with summary: {with_summary}/{total}")
                    print(f"      Posts with tags: {with_tags}/{total}")
                    print(f"      Posts with Ghost URL: {with_ghost_url}/{total}")
                    print(f"      Published posts: {published}/{total}")
                    
                    quality_score = 0
                    if total > 0:
                        quality_score = ((with_summary + with_tags + with_ghost_url) / (total * 3)) * 100
                    
                    print(f"   ğŸ“ˆ Data Quality Score: {quality_score:.1f}%")
                    
                    return {
                        "quality_check": True,
                        "total_posts": total,
                        "quality_score": quality_score,
                        "published_posts": published
                    }
        
        print(f"   âš ï¸ Data quality check failed")
        return {"quality_check": False}
        
    except Exception as e:
        print(f"   âš ï¸ Data quality check error: {e}")
        return {"quality_check": False, "error": str(e)}

def run_full_pipeline_integration_test():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print_header("ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸")
    print(f"ğŸ• Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = {
        "system_health": False,
        "collection_completed": False,
        "processing_completed": False,
        "publishing_completed": False,
        "pipeline_success": False,
        "data_quality": False
    }
    
    # 1. ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬
    print_step("1. System Health Check")
    health_status = check_system_health()
    test_results["system_health"] = health_status.get("healthy", False)
    
    if not test_results["system_health"]:
        print("âš ï¸ System health check failed, but continuing with integration test...")
        print("   (Database may be experiencing temporary issues but is functional)")
    
    # 2. ì´ˆê¸° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    print_step("2. Initial Metrics Collection")
    initial_metrics = get_initial_metrics()
    
    # 3. Reddit ìˆ˜ì§‘ ë‹¨ê³„
    print_step("3. Reddit Collection Phase")
    collection_trigger = trigger_reddit_collection()
    
    if not collection_trigger:
        print("âŒ Failed to trigger Reddit collection. Aborting test.")
        return False
    
    test_results["collection_completed"] = wait_for_collection_completion(300)
    
    if not test_results["collection_completed"]:
        print("âŒ Reddit collection did not complete. Continuing with existing data...")
    
    # 4. AI ì²˜ë¦¬ ë‹¨ê³„
    print_step("4. AI Processing Phase")
    processing_trigger = trigger_ai_processing()
    
    if not processing_trigger:
        print("âŒ Failed to trigger AI processing. Aborting test.")
        return False
    
    test_results["processing_completed"] = wait_for_processing_completion(600)
    
    if not test_results["processing_completed"]:
        print("âŒ AI processing did not complete. Continuing with existing data...")
    
    # 5. Ghost ë°œí–‰ ë‹¨ê³„
    print_step("5. Ghost Publishing Phase")
    publishing_trigger = trigger_ghost_publishing()
    
    if not publishing_trigger:
        print("âŒ Failed to trigger Ghost publishing. Aborting test.")
        return False
    
    test_results["publishing_completed"] = wait_for_publishing_completion(300)
    
    if not test_results["publishing_completed"]:
        print("âŒ Ghost publishing did not complete.")
    
    # 6. ìµœì¢… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ê²°ê³¼ ê²€ì¦
    print_step("6. Results Verification")
    final_metrics = get_final_metrics()
    
    pipeline_results = verify_pipeline_results(initial_metrics, final_metrics)
    test_results["pipeline_success"] = pipeline_results.get("overall_success", False)
    
    # 7. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    print_step("7. Data Quality Verification")
    quality_results = check_data_quality()
    test_results["data_quality"] = quality_results.get("quality_check", False)
    
    # 8. í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
    print_header("ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    
    passed_tests = sum(test_results.values())
    total_tests = len(test_results)
    
    print(f"ğŸ“Š Integration Test Results: {passed_tests}/{total_tests} passed")
    
    for test_name, passed in test_results.items():
        emoji = "âœ…" if passed else "âŒ"
        test_display = test_name.replace('_', ' ').title()
        print(f"   {emoji} {test_display}: {'PASS' if passed else 'FAIL'}")
    
    # íŒŒì´í”„ë¼ì¸ í†µê³„
    if pipeline_results.get("stats"):
        stats = pipeline_results["stats"]
        print(f"\nğŸ“ˆ Pipeline Statistics:")
        print(f"   ğŸ“¥ Posts Collected: {stats.get('collected', 0)}")
        print(f"   ğŸ¤– Posts Processed: {stats.get('processed', 0)}")
        print(f"   ğŸ‘» Posts Published: {stats.get('published', 0)}")
        print(f"   ğŸ“Š Success Rate: {stats.get('success_rate', 0):.1f}%")
        
        if stats.get('collection_failures', 0) > 0:
            print(f"   âŒ Collection Failures: {stats['collection_failures']}")
        if stats.get('processing_failures', 0) > 0:
            print(f"   âŒ Processing Failures: {stats['processing_failures']}")
        if stats.get('publishing_failures', 0) > 0:
            print(f"   âŒ Publishing Failures: {stats['publishing_failures']}")
    
    # ë°ì´í„° í’ˆì§ˆ ê²°ê³¼
    if quality_results.get("quality_check"):
        print(f"\nğŸ“‹ Data Quality Results:")
        print(f"   ğŸ“Š Quality Score: {quality_results.get('quality_score', 0):.1f}%")
        print(f"   ğŸ“ Total Posts: {quality_results.get('total_posts', 0)}")
        print(f"   ğŸ‘» Published Posts: {quality_results.get('published_posts', 0)}")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ”§ Recommendations:")
    if test_results["pipeline_success"]:
        print("   âœ… Full pipeline integration is working correctly!")
        print("   ğŸ”„ Ready for performance testing and production deployment.")
    else:
        print("   âš ï¸ Pipeline integration has issues that need attention:")
        
        if not test_results["collection_completed"]:
            print("   ğŸ“¥ Check Reddit API configuration and rate limits")
        if not test_results["processing_completed"]:
            print("   ğŸ¤– Check OpenAI API configuration and token limits")
        if not test_results["publishing_completed"]:
            print("   ğŸ‘» Check Ghost CMS configuration and connectivity")
        if not test_results["data_quality"]:
            print("   ğŸ“‹ Check data processing logic and database integrity")
    
    # ì „ì²´ ì„±ê³µ ê¸°ì¤€: ìµœì†Œí•œ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
    overall_success = test_results["pipeline_success"] and test_results["system_health"]
    
    return overall_success

if __name__ == "__main__":
    try:
        success = run_full_pipeline_integration_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Integration test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Integration test failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)