"""
OpenAI GPT-4o-mini/GPT-4o client for Reddit Ghost Publisher MVP
Simplified implementation with fallback support and cost tracking
"""
import logging
from typing import Dict, Any, Optional, Tuple
from decimal import Decimal
from datetime import datetime, timedelta
import json
import os

import openai
from openai import OpenAI
from openai.types.chat import ChatCompletion

from app.config import get_settings
from app.redis_client import redis_client

logger = logging.getLogger(__name__)
settings = get_settings()


class OpenAIClient:
    """Simplified OpenAI client with GPT-4o-mini primary and GPT-4o fallback"""
    
    # Internal cost map (per 1K tokens)
    COST_PER_1K_TOKENS = {
        'gpt-4o-mini': Decimal('0.00015'),  # $0.15/1M input tokens
        'gpt-4o': Decimal('0.005')          # $5.00/1M input tokens  
    }
    
    def __init__(self):
        self._client: Optional[OpenAI] = None
        self._api_key = os.getenv('OPENAI_API_KEY')
        self._daily_token_limit = int(os.getenv('OPENAI_DAILY_TOKENS_LIMIT', '100000'))
        
        # Model configuration: primary + fallback
        self._primary_model = 'gpt-4o-mini'
        self._fallback_model = 'gpt-4o'
    
    def initialize(self) -> None:
        """Initialize OpenAI client (synchronous for MVP)"""
        if not self._api_key:
            raise ValueError("OpenAI API key not found in environment variables")
        
        self._client = OpenAI(api_key=self._api_key)
        logger.info("OpenAI client initialized successfully")
    
    def check_daily_token_usage(self) -> Tuple[int, bool]:
        """
        Check daily token usage against limit with 80%/100% alerts
        
        Returns:
            Tuple of (current_usage, is_over_limit)
        """
        try:
            today = datetime.utcnow().strftime("%Y%m%d")
            cache_key = f"usage:{today}"
            
            current_usage = redis_client.get(cache_key)
            current_usage = int(current_usage) if current_usage else 0
            
            # Check for alert thresholds
            usage_percentage = (current_usage / self._daily_token_limit) * 100
            
            # 80% threshold alert
            if usage_percentage >= 80 and not self._has_sent_alert(today, "80"):
                self._send_token_budget_alert(current_usage, self._daily_token_limit, "80%")
                self._mark_alert_sent(today, "80")
            
            # 100% threshold alert
            is_over_limit = current_usage >= self._daily_token_limit
            if is_over_limit and not self._has_sent_alert(today, "100"):
                self._send_token_budget_alert(current_usage, self._daily_token_limit, "100%")
                self._mark_alert_sent(today, "100")
            
            return current_usage, is_over_limit
            
        except Exception as e:
            logger.error(f"Failed to check daily token usage: {e}")
            return 0, False
    
    def _has_sent_alert(self, date: str, threshold: str) -> bool:
        """Check if alert has been sent for this date and threshold"""
        try:
            alert_key = f"alert_sent:{date}:{threshold}"
            return redis_client.exists(alert_key)
        except Exception:
            return False
    
    def _mark_alert_sent(self, date: str, threshold: str) -> None:
        """Mark alert as sent for this date and threshold"""
        try:
            alert_key = f"alert_sent:{date}:{threshold}"
            redis_client.setex(alert_key, 86400, "1")  # Expire in 24 hours
        except Exception as e:
            logger.error(f"Failed to mark alert as sent: {e}")
    
    def _send_token_budget_alert(self, current_usage: int, limit: int, threshold: str) -> None:
        """Send Slack alert for token budget threshold"""
        try:
            import requests
            
            slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
            if not slack_webhook_url:
                logger.warning("SLACK_WEBHOOK_URL not configured, skipping alert")
                return
            
            usage_percentage = (current_usage / limit) * 100
            
            payload = {
                "text": f"ğŸš¨ [HIGH] OpenAI Token Budget Alert",
                "attachments": [
                    {
                        "color": "danger" if threshold == "100%" else "warning",
                        "fields": [
                            {"title": "Service", "value": "OpenAI API", "short": True},
                            {"title": "Threshold", "value": threshold, "short": True},
                            {"title": "Current Usage", "value": f"{current_usage:,} tokens", "short": True},
                            {"title": "Daily Limit", "value": f"{limit:,} tokens", "short": True},
                            {"title": "Usage %", "value": f"{usage_percentage:.1f}%", "short": True},
                            {"title": "Status", "value": "BLOCKED" if threshold == "100%" else "WARNING", "short": True}
                        ]
                    }
                ]
            }
            
            response = requests.post(slack_webhook_url, json=payload, timeout=10)
            if response.status_code == 200:
                logger.info(f"Token budget alert sent for {threshold} threshold")
            else:
                logger.error(f"Failed to send Slack alert: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Failed to send token budget alert: {e}")
    
    def _update_token_usage(self, tokens_used: int) -> None:
        """Update daily token usage counter with UTC 00:00 auto reset"""
        try:
            today = datetime.utcnow().strftime("%Y%m%d")
            cache_key = f"usage:{today}"
            
            # Increment counter
            redis_client.incr(cache_key, tokens_used)
            
            # Set expiry to end of day UTC if key is new
            ttl = redis_client.ttl(cache_key)
            if ttl == -1:  # Key exists but no expiry set
                tomorrow = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
                seconds_until_tomorrow = int((tomorrow - datetime.utcnow()).total_seconds())
                redis_client.expire(cache_key, seconds_until_tomorrow)
            
        except Exception as e:
            logger.error(f"Failed to update token usage: {e}")
    
    def _calculate_cost(self, total_tokens: int, model: str) -> Decimal:
        """Calculate cost using internal cost map"""
        cost_per_1k = self.COST_PER_1K_TOKENS.get(model, Decimal('0.001'))
        return (Decimal(total_tokens) / 1000) * cost_per_1k
    
    def _get_korean_summary_prompt(self, title: str, content: str) -> str:
        """Generate Korean summary prompt template"""
        return f"""ë‹¤ìŒ Reddit ê²Œì‹œê¸€ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title}

ë‚´ìš©:
{content}

ìš”ì•½ ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ ë‚´ìš©ì„ 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½
2. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
3. ì›ë¬¸ì˜ ë§¥ë½ê³¼ í†¤ì„ ìœ ì§€
4. ê¸°ìˆ ì  ìš©ì–´ëŠ” í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë˜ í•„ìš”ì‹œ ì˜ì–´ ë³‘ê¸°
5. ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ í†¤ ìœ ì§€

ìš”ì•½:"""

    def generate_korean_summary(
        self, 
        post_title: str, 
        post_content: str, 
        post_id: str,
        max_tokens: int = 500
    ) -> Dict[str, Any]:
        """
        Generate Korean summary with GPT-4o-mini primary + GPT-4o fallback
        
        Args:
            post_title: Reddit post title
            post_content: Reddit post content
            post_id: Post ID for tracking
            max_tokens: Maximum tokens for response
        
        Returns:
            Dictionary with summary and token usage info
        """
        if not self._client:
            self.initialize()
        
        # Check daily token limit
        current_usage, is_over_limit = self.check_daily_token_usage()
        if is_over_limit:
            raise Exception(f"Daily token limit exceeded: {current_usage}/{self._daily_token_limit}")
        
        # Prepare Korean summary prompt
        prompt = self._get_korean_summary_prompt(post_title, post_content)
        
        # Try primary model first (GPT-4o-mini) with enhanced error handling
        model_used = None
        response = None
        fallback_attempted = False
        
        for attempt in range(2):  # Primary + fallback attempt
            current_model = self._primary_model if attempt == 0 else self._fallback_model
            
            try:
                logger.debug(f"Attempting {current_model} for post {post_id} (attempt {attempt + 1})")
                
                response = self._client.chat.completions.create(
                    model=current_model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ Reddit ê²Œì‹œê¸€ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°„ê²°í•˜ë©° ì´í•´í•˜ê¸° ì‰¬ìš´ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.3,
                    timeout=30  # Add timeout
                )
                
                model_used = current_model
                if attempt == 0:
                    logger.info(f"Used primary model {self._primary_model} for post {post_id}")
                else:
                    logger.info(f"Used fallback model {self._fallback_model} for post {post_id}")
                    fallback_attempted = True
                break
                
            except openai.RateLimitError as e:
                logger.warning(f"Rate limit error with {current_model} for post {post_id}: {e}")
                if attempt == 0:  # Try fallback on rate limit
                    logger.info(f"Rate limit on primary model, trying fallback {self._fallback_model}")
                    continue
                else:
                    # Both models rate limited, raise the error
                    raise Exception(f"Both models rate limited: {e}")
                    
            except openai.APIError as e:
                logger.error(f"API error with {current_model} for post {post_id}: {e}")
                if attempt == 0 and "model_not_found" not in str(e).lower():
                    logger.info(f"API error on primary model, trying fallback {self._fallback_model}")
                    continue
                else:
                    raise Exception(f"API error: {e}")
                    
            except openai.APITimeoutError as e:
                logger.warning(f"Timeout error with {current_model} for post {post_id}: {e}")
                if attempt == 0:
                    logger.info(f"Timeout on primary model, trying fallback {self._fallback_model}")
                    continue
                else:
                    raise Exception(f"Timeout on both models: {e}")
                    
            except openai.APIConnectionError as e:
                logger.warning(f"Connection error with {current_model} for post {post_id}: {e}")
                if attempt == 0:
                    logger.info(f"Connection error on primary model, trying fallback {self._fallback_model}")
                    continue
                else:
                    raise Exception(f"Connection error on both models: {e}")
                    
            except Exception as e:
                logger.error(f"Unexpected error with {current_model} for post {post_id}: {e}")
                if attempt == 0:
                    logger.info(f"Unexpected error on primary model, trying fallback {self._fallback_model}")
                    continue
                else:
                    raise Exception(f"Both models failed: {e}")
        
        if not response or not model_used:
            raise Exception("Failed to get response from both primary and fallback models")
        
        # Extract response data
        summary = response.choices[0].message.content.strip()
        total_tokens = response.usage.total_tokens
        
        # Calculate cost using internal cost map
        cost = self._calculate_cost(total_tokens, model_used)
        
        # Update token usage tracking
        self._update_token_usage(total_tokens)
        
        logger.info(
            f"Generated Korean summary for post {post_id}",
            extra={
                "post_id": post_id,
                "model": model_used,
                "total_tokens": total_tokens,
                "cost_usd": float(cost),
                "summary_length": len(summary)
            }
        )
        
        return {
            "summary": summary,
            "model": model_used,
            "total_tokens": total_tokens,
            "cost_usd": cost
        }
    
    def extract_tags_llm(
        self,
        post_title: str,
        post_content: str,
        post_id: str,
        max_tokens: int = 200
    ) -> Dict[str, Any]:
        """
        Extract 3-5 tags using LLM prompt with consistent formatting rules
        
        Args:
            post_title: Reddit post title
            post_content: Reddit post content
            post_id: Post ID for tracking
            max_tokens: Maximum tokens for response
        
        Returns:
            Dictionary with tags and token usage info
        """
        if not self._client:
            self.initialize()
        
        # Check daily token limit
        current_usage, is_over_limit = self.check_daily_token_usage()
        if is_over_limit:
            raise Exception(f"Daily token limit exceeded: {current_usage}/{self._daily_token_limit}")
        
        # Prepare tag extraction prompt
        prompt = self._get_tag_extraction_prompt(post_title, post_content)
        
        # Try primary model first (GPT-4o-mini) with enhanced error handling
        model_used = None
        response = None
        
        for attempt in range(2):  # Primary + fallback attempt
            current_model = self._primary_model if attempt == 0 else self._fallback_model
            
            try:
                logger.debug(f"Attempting {current_model} for tag extraction {post_id} (attempt {attempt + 1})")
                
                response = self._client.chat.completions.create(
                    model=current_model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ Reddit ê²Œì‹œê¸€ì—ì„œ ê²€ìƒ‰ ìµœì í™”ëœ íƒœê·¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¼ê´€ëœ í‘œê¸° ê·œì¹™ì„ ë”°ë¼ 3-5ê°œì˜ íƒœê·¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.2,
                    timeout=30
                )
                
                model_used = current_model
                if attempt == 0:
                    logger.info(f"Used primary model {self._primary_model} for tag extraction {post_id}")
                else:
                    logger.info(f"Used fallback model {self._fallback_model} for tag extraction {post_id}")
                break
                
            except openai.RateLimitError as e:
                logger.warning(f"Rate limit error with {current_model} for tag extraction {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Both models rate limited for tag extraction: {e}")
                    
            except openai.APIError as e:
                logger.error(f"API error with {current_model} for tag extraction {post_id}: {e}")
                if attempt == 0 and "model_not_found" not in str(e).lower():
                    continue
                else:
                    raise Exception(f"API error in tag extraction: {e}")
                    
            except openai.APITimeoutError as e:
                logger.warning(f"Timeout error with {current_model} for tag extraction {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Timeout on both models for tag extraction: {e}")
                    
            except openai.APIConnectionError as e:
                logger.warning(f"Connection error with {current_model} for tag extraction {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Connection error on both models for tag extraction: {e}")
                    
            except Exception as e:
                logger.error(f"Unexpected error with {current_model} for tag extraction {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Both models failed for tag extraction: {e}")
        
        if not response or not model_used:
            raise Exception("Failed to get response from both models for tag extraction")
        
        # Extract and parse tags
        tags_text = response.choices[0].message.content.strip()
        total_tokens = response.usage.total_tokens
        
        # Parse tags from response (expecting comma-separated format)
        tags = []
        for tag in tags_text.split(','):
            tag = tag.strip().lower()
            if tag and len(tags) < 5:  # Limit to 5 tags
                tags.append(tag)
        
        # Ensure we have at least 3 tags
        if len(tags) < 3:
            # Add generic tags if needed
            generic_tags = ['reddit', 'ê²Œì‹œê¸€', 'ì½˜í…ì¸ ']
            for generic_tag in generic_tags:
                if generic_tag not in tags and len(tags) < 3:
                    tags.append(generic_tag)
        
        # Calculate cost using internal cost map
        cost = self._calculate_cost(total_tokens, model_used)
        
        # Update token usage tracking
        self._update_token_usage(total_tokens)
        
        logger.info(
            f"Extracted tags for post {post_id}",
            extra={
                "post_id": post_id,
                "model": model_used,
                "total_tokens": total_tokens,
                "cost_usd": float(cost),
                "tags_count": len(tags),
                "tags": tags
            }
        )
        
        return {
            "tags": tags,
            "model": model_used,
            "total_tokens": total_tokens,
            "cost_usd": cost
        }
    
    def _get_tag_extraction_prompt(self, title: str, content: str) -> str:
        """Generate tag extraction prompt template"""
        return f"""ë‹¤ìŒ Reddit ê²Œì‹œê¸€ì—ì„œ 3-5ê°œì˜ íƒœê·¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œëª©: {title}

ë‚´ìš©:
{content}

íƒœê·¸ ì¶”ì¶œ ìš”êµ¬ì‚¬í•­:
1. 3ê°œì—ì„œ 5ê°œì˜ íƒœê·¸ ì¶”ì¶œ
2. ëª¨ë“  íƒœê·¸ëŠ” ì†Œë¬¸ìë¡œ ì‘ì„±
3. í•œê¸€ íƒœê·¸ ìš°ì„ , í•„ìš”ì‹œ ì˜ì–´ ì‚¬ìš©
4. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ì„ íƒ
5. ì¼ê´€ëœ í‘œê¸° ê·œì¹™ ì ìš© (ë„ì–´ì“°ê¸° ì—†ì´, í•˜ì´í”ˆ ì‚¬ìš© ê°€ëŠ¥)
6. ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´

ì˜ˆì‹œ: ê°œë°œ, í”„ë¡œê·¸ë˜ë°, ì›¹ê°œë°œ, ê¸°ìˆ , íŠœí† ë¦¬ì–¼

íƒœê·¸:"""
    
    def analyze_pain_points_and_ideas(
        self,
        post_title: str,
        post_content: str,
        post_id: str,
        max_tokens: int = 800
    ) -> Dict[str, Any]:
        """
        Analyze pain points and product ideas with JSON schema validation
        
        Args:
            post_title: Reddit post title
            post_content: Reddit post content
            post_id: Post ID for tracking
            max_tokens: Maximum tokens for response
        
        Returns:
            Dictionary with analysis results and token usage info
        """
        if not self._client:
            self.initialize()
        
        # Check daily token limit
        current_usage, is_over_limit = self.check_daily_token_usage()
        if is_over_limit:
            raise Exception(f"Daily token limit exceeded: {current_usage}/{self._daily_token_limit}")
        
        # Prepare analysis prompt
        prompt = self._get_pain_points_analysis_prompt(post_title, post_content)
        
        # Try primary model first (GPT-4o-mini) with enhanced error handling
        model_used = None
        response = None
        
        for attempt in range(2):  # Primary + fallback attempt
            current_model = self._primary_model if attempt == 0 else self._fallback_model
            
            try:
                logger.debug(f"Attempting {current_model} for analysis {post_id} (attempt {attempt + 1})")
                
                response = self._client.chat.completions.create(
                    model=current_model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ Reddit ê²Œì‹œê¸€ì—ì„œ ì‚¬ìš©ìì˜ í˜ì¸ í¬ì¸íŠ¸ì™€ ì œí’ˆ ì•„ì´ë””ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.2,
                    response_format={"type": "json_object"},
                    timeout=30
                )
                
                model_used = current_model
                if attempt == 0:
                    logger.info(f"Used primary model {self._primary_model} for analysis {post_id}")
                else:
                    logger.info(f"Used fallback model {self._fallback_model} for analysis {post_id}")
                break
                
            except openai.RateLimitError as e:
                logger.warning(f"Rate limit error with {current_model} for analysis {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Both models rate limited for analysis: {e}")
                    
            except openai.APIError as e:
                logger.error(f"API error with {current_model} for analysis {post_id}: {e}")
                if attempt == 0 and "model_not_found" not in str(e).lower():
                    continue
                else:
                    raise Exception(f"API error in analysis: {e}")
                    
            except openai.APITimeoutError as e:
                logger.warning(f"Timeout error with {current_model} for analysis {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Timeout on both models for analysis: {e}")
                    
            except openai.APIConnectionError as e:
                logger.warning(f"Connection error with {current_model} for analysis {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Connection error on both models for analysis: {e}")
                    
            except Exception as e:
                logger.error(f"Unexpected error with {current_model} for analysis {post_id}: {e}")
                if attempt == 0:
                    continue
                else:
                    raise Exception(f"Both models failed for analysis: {e}")
        
        if not response or not model_used:
            raise Exception("Failed to get response from both models for analysis")
        
        # Extract and parse response
        analysis_text = response.choices[0].message.content.strip()
        total_tokens = response.usage.total_tokens
        
        # Parse JSON response with schema validation
        try:
            analysis_data = json.loads(analysis_text)
            # Validate and ensure required schema fields
            analysis_data = self._validate_analysis_schema(analysis_data)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse analysis JSON for post {post_id}: {e}")
            # Fallback structure with schema compliance
            analysis_data = self._get_fallback_analysis_schema()
        
        # Calculate cost using internal cost map
        cost = self._calculate_cost(total_tokens, model_used)
        
        # Update token usage tracking
        self._update_token_usage(total_tokens)
        
        logger.info(
            f"Analyzed pain points for post {post_id}",
            extra={
                "post_id": post_id,
                "model": model_used,
                "total_tokens": total_tokens,
                "cost_usd": float(cost),
                "pain_points_count": len(analysis_data.get("pain_points", [])),
                "product_ideas_count": len(analysis_data.get("product_ideas", []))
            }
        )
        
        return {
            "analysis": analysis_data,
            "model": model_used,
            "total_tokens": total_tokens,
            "cost_usd": cost
        }
    
    def _get_pain_points_analysis_prompt(self, title: str, content: str) -> str:
        """Generate pain points analysis prompt template with JSON schema"""
        return f"""ë‹¤ìŒ Reddit ê²Œì‹œê¸€ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ í˜ì¸ í¬ì¸íŠ¸ì™€ ì ì¬ì  ì œí’ˆ ì•„ì´ë””ì–´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œëª©: {title}

ë‚´ìš©:
{content}

ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶° ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{{
  "meta": {{
    "version": "1.0",
    "analysis_date": "{datetime.utcnow().isoformat()}",
    "confidence_score": 0.85
  }},
  "pain_points": [
    {{
      "description": "í˜ì¸ í¬ì¸íŠ¸ ì„¤ëª…",
      "category": "ì¹´í…Œê³ ë¦¬ (ê¸°ìˆ ì /ì‚¬ìš©ì„±/ë¹„ìš©/ì‹œê°„/ê¸°íƒ€)",
      "severity": "ì‹¬ê°ë„ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)",
      "frequency": "ë¹ˆë„ (ìì£¼/ê°€ë”/ë“œë¬¼ê²Œ)"
    }}
  ],
  "product_ideas": [
    {{
      "title": "ì œí’ˆ ì•„ì´ë””ì–´ ì œëª©",
      "description": "ì œí’ˆ ì•„ì´ë””ì–´ ì„¤ëª…",
      "target_pain_point": "í•´ê²°í•˜ëŠ” í˜ì¸ í¬ì¸íŠ¸",
      "feasibility": "ì‹¤í˜„ ê°€ëŠ¥ì„± (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)",
      "market_potential": "ì‹œì¥ ì ì¬ë ¥ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)"
    }}
  ],
  "analysis_notes": "ë¶„ì„ ê³¼ì •ì—ì„œì˜ ì¶”ê°€ ë…¸íŠ¸"
}}

ë¶„ì„ ê¸°ì¤€:
1. ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ë¬¸ì œì ê³¼ ë¶ˆë§Œì‚¬í•­ ì‹ë³„
2. ì•”ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ëŠ” ë‹ˆì¦ˆì™€ ê°œì„ ì  íŒŒì•…
3. ì‹¤í˜„ ê°€ëŠ¥í•œ ì œí’ˆ/ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ ë„ì¶œ
4. ì‹œì¥ì„±ê³¼ ê¸°ìˆ ì  ì‹¤í˜„ ê°€ëŠ¥ì„± ê³ ë ¤
5. meta.version í•„ë“œëŠ” ë°˜ë“œì‹œ í¬í•¨"""
    
    def _validate_analysis_schema(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and ensure JSON schema compliance"""
        # Ensure meta field exists with version
        if "meta" not in data:
            data["meta"] = {}
        
        if "version" not in data["meta"]:
            data["meta"]["version"] = "1.0"
        
        if "analysis_date" not in data["meta"]:
            data["meta"]["analysis_date"] = datetime.utcnow().isoformat()
        
        # Ensure required arrays exist
        if "pain_points" not in data:
            data["pain_points"] = []
        
        if "product_ideas" not in data:
            data["product_ideas"] = []
        
        # Validate pain_points structure
        for pain_point in data["pain_points"]:
            if not isinstance(pain_point, dict):
                continue
            pain_point.setdefault("description", "")
            pain_point.setdefault("category", "ê¸°íƒ€")
            pain_point.setdefault("severity", "ë³´í†µ")
            pain_point.setdefault("frequency", "ê°€ë”")
        
        # Validate product_ideas structure
        for idea in data["product_ideas"]:
            if not isinstance(idea, dict):
                continue
            idea.setdefault("title", "")
            idea.setdefault("description", "")
            idea.setdefault("target_pain_point", "")
            idea.setdefault("feasibility", "ë³´í†µ")
            idea.setdefault("market_potential", "ë³´í†µ")
        
        return data
    
    def _get_fallback_analysis_schema(self) -> Dict[str, Any]:
        """Get fallback analysis structure when JSON parsing fails"""
        return {
            "meta": {
                "version": "1.0",
                "analysis_date": datetime.utcnow().isoformat(),
                "confidence_score": 0.0,
                "parsing_error": True
            },
            "pain_points": [],
            "product_ideas": [],
            "analysis_notes": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜"
        }
    
    def health_check(self) -> Dict[str, Any]:
        """Check OpenAI client health status"""
        try:
            if not self._client:
                return {
                    "status": "unhealthy",
                    "error": "Client not initialized",
                    "api_key_configured": bool(self._api_key)
                }
            
            # Check daily token usage
            current_usage, is_over_limit = self.check_daily_token_usage()
            
            return {
                "status": "healthy" if not is_over_limit else "degraded",
                "daily_token_usage": current_usage,
                "daily_token_limit": self._daily_token_limit,
                "over_limit": is_over_limit,
                "primary_model": self._primary_model,
                "fallback_model": self._fallback_model
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }


# Global OpenAI client instance
openai_client = OpenAIClient()


def get_openai_client() -> OpenAIClient:
    """Get the global OpenAI client instance"""
    return openai_client